{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Xavier initialization\n",
    "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network.\n",
    "\n",
    "If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all.\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between:\n",
    "    sqrt(6) / sqrt(n_i + n_o)\n",
    "    \n",
    "#### Even better?\n",
    "\n",
    "In their 2015 paper, He et. al. demonstrated that deep networks (e.g. a 22-layer CNN) would converge much earlier if the following input weight initialization strategy is employed:\n",
    "\n",
    "  - Create a tensor with the dimensions appropriate for a weight matrix at a given layer, and populate it with numbers randomly chosen from a standard normal distribution.\n",
    "  - Multiply each randomly chosen number by √2/√n where n is the number of incoming connections coming into a given layer from the previous layer’s output (also known as the “fan-in”).\n",
    "  - Bias tensors are initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define environment and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# create cart pole environment\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# get main parameters from the agent-environment interaction\n",
    "\n",
    "number_of_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "H = 8 \n",
    "A = number_of_actions\n",
    "S = state_dim\n",
    "\n",
    "# Model weights\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define useful functions for training and evaluating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    return np.exp(y)/np.sum(np.exp(y))\n",
    "\n",
    "def propagate_forward(x):\n",
    "    global model\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0\n",
    "    y = np.dot(model['W2'], h)\n",
    "    p = softmax(y)\n",
    "\n",
    "    return h,y,p\n",
    "\n",
    "def policy_gradient(action, reward, y, p):\n",
    "    grad = -p\n",
    "    grad[action] += 1.\n",
    "    return grad*reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input\n",
    "x = np.array([-0.18903734, -1.75218701,  0.21354577,  2.74911676])\n",
    "\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are lucky that as we have dense layers, the gradient is linear\n",
    "\n",
    "# W2 update\n",
    "dw2 = np.outer(dy, h)\n",
    "## that means that if we ONLY update W2 += dw2, now forward propagating\n",
    "## the same input x will yield the 'perfect' policy value for this example (proof below)\n",
    "## W2 ready :)\n",
    "\n",
    "\n",
    "\n",
    "# W1 update\n",
    "dh = np.dot(dy, model['W2'])\n",
    "dh[h==0] = 0\n",
    "## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "dw1 = np.outer(dh, x)\n",
    "## W1 ready :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BACKPROP Tests: \n",
    "#### 1. After only updating second layer weights and evaluating policy gradient again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.50281072 -2.51491859  1.45652641 -0.58305546  0.67784589 -0.55232407\n",
      "  -0.06772914 -1.20852512]\n",
      " [ 1.6629324   2.69174192 -1.36031205  0.92222555 -1.04027333  0.42643155\n",
      "  -0.45370185  1.2694156 ]]\n",
      "[ 1. -1.]\n",
      "[ 0.00000000e+00 -5.39829682e-55]\n"
     ]
    }
   ],
   "source": [
    "print(model['W2'])\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)\n",
    "\n",
    "# W2 update\n",
    "dw2 = np.outer(dy, h)\n",
    "model['W2'] += dw2\n",
    "\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The policy gradient is 0 :)\n",
    "\n",
    "\n",
    "##### Of course, this update is crazy, as you force to perfectly fit the optimal policy for the current example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After only updating W1 and evaluating policy gradient again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11081384 -0.11081384]\n",
      "[ 0.02402039 -0.02402039]\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H)\n",
    "\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "\n",
    "print(dy)\n",
    "\n",
    "# W1 update\n",
    "dh = np.dot(dy, model['W2'])\n",
    "dh[h==0] = 0\n",
    "## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "dw1 = np.outer(dh, x)\n",
    "## W1 ready :)\n",
    "\n",
    "model['W1'] += dw1\n",
    "\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The policy gradient is not 0 because the RELU keeps the necessary neurons from activating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anyway, the final backprop code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(dy, h, x):\n",
    "    global model\n",
    "    # we are lucky that as we have dense layers, the gradient is linear\n",
    "\n",
    "    # W2 update\n",
    "    dw2 = np.outer(dy, h)\n",
    "    ## that means that if we ONLY update W2 += dw2, now forward propagating\n",
    "    ## the same input x will yield the 'perfect' policy value for this example (proof below)\n",
    "    ## W2 ready :)\n",
    "\n",
    "    # W1 update\n",
    "    dh = np.dot(dy, model['W2'])\n",
    "    dh[h<=0] = 0\n",
    "    ## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "    dw1 = np.outer(dh, x)\n",
    "    ## W1 ready :)\n",
    "    return dw1, dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Rewards\n",
    "As in Karpathy's Pong from Pixels, we normalize the rewards obtained in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "gamma_series = [1]\n",
    "for e in range(1000):\n",
    "    gamma_series.append(gamma_series[-1]*gamma + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode lasted  21.006\n",
      "Average episode lasted  26.03\n",
      "Average episode lasted  36.168\n",
      "Average episode lasted  52.74\n",
      "Average episode lasted  84.686\n",
      "Average episode lasted  140.126\n",
      "Average episode lasted  95.25\n",
      "Average episode lasted  196.574\n",
      "Average episode lasted  352.36\n",
      "Average episode lasted  252.646\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S*H) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H*A)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(10):\n",
    "# Play a bunch of episodes\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "\n",
    "\n",
    "    for e in range(500):\n",
    "        terminal = False\n",
    "        state = env.reset()\n",
    "        r = 0\n",
    "\n",
    "        while not terminal:\n",
    "\n",
    "            h,y,p = propagate_forward(state)\n",
    "\n",
    "            states.append(state)\n",
    "\n",
    "\n",
    "\n",
    "            # select action for current state\n",
    "            action = np.random.choice(2, p=p)\n",
    "            actions.append(action)\n",
    "\n",
    "            state, reward, terminal, info = env.step(action)\n",
    "\n",
    "            r+= 1\n",
    "\n",
    "\n",
    "        rewards.append(r)\n",
    "    print('Average episode lasted ',np.mean(rewards))\n",
    "\n",
    "    rs = [list(reversed(gamma_series[:r])) for r in rewards]\n",
    "    mean_r = np.mean([np.mean(r) for r in rs])\n",
    "    rs = np.concatenate(rs)\n",
    "    normalized_rewards = (rs - np.mean(rs))/np.std(rs)\n",
    "\n",
    "\n",
    "    # train\n",
    "\n",
    "    ind = np.arange(len(rs))\n",
    "    np.random.shuffle(ind)\n",
    "    for t in ind:\n",
    "        reward = normalized_rewards[t]\n",
    "        state = states[t]\n",
    "        action = actions[t]\n",
    "\n",
    "        h,y,p = propagate_forward(state)\n",
    "        dy = policy_gradient(action,reward, y, p)\n",
    "        dw1, dw2 = backprop(dy, h, state)\n",
    "        \n",
    "        #update weights (train)\n",
    "        \n",
    "        model['W1'] += np.clip(learning_rate * dw1, -0.0005, +0.0005)\n",
    "        model['W2'] += np.clip(learning_rate * dw2, -0.0005, +0.0005)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See the model play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see it play\n",
    "!python see_the_model_play.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
