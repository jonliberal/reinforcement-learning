{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Xavier initialization\n",
    "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network.\n",
    "\n",
    "If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all.\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between:\n",
    "    sqrt(6) / sqrt(n_i + n_o)\n",
    "    \n",
    "#### Even better?\n",
    "\n",
    "In their 2015 paper, He et. al. demonstrated that deep networks (e.g. a 22-layer CNN) would converge much earlier if the following input weight initialization strategy is employed:\n",
    "\n",
    "  - Create a tensor with the dimensions appropriate for a weight matrix at a given layer, and populate it with numbers randomly chosen from a standard normal distribution.\n",
    "  - Multiply each randomly chosen number by √2/√n where n is the number of incoming connections coming into a given layer from the previous layer’s output (also known as the “fan-in”).\n",
    "  - Bias tensors are initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define environment and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create cart pole environment\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# get main parameters from the agent-environment interaction\n",
    "\n",
    "number_of_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "H = 8 \n",
    "A = number_of_actions\n",
    "S = state_dim\n",
    "\n",
    "# Model weights\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define useful functions for training and evaluating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    if max(y) > 10:\n",
    "        print(\"WARNING: NUMEROS GORDACOS\")\n",
    "        print(y)\n",
    "    return np.exp(y)/np.sum(np.exp(y))\n",
    "\n",
    "def propagate_forward(x):\n",
    "    global model\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0\n",
    "    y = np.dot(model['W2'], h)\n",
    "    p = softmax(y)\n",
    "\n",
    "    return h,y,p\n",
    "\n",
    "def policy_gradient(action, reward, y, p):\n",
    "    grad = -p\n",
    "    grad[action] += 1.\n",
    "    return grad*reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input\n",
    "x = np.array([-0.18903734, -1.75218701,  0.21354577,  2.74911676])\n",
    "\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are lucky that as we have dense layers, the gradient is linear\n",
    "\n",
    "# W2 update\n",
    "dw2 = np.outer(dy, h)\n",
    "## that means that if we ONLY update W2 += dw2, now forward propagating\n",
    "## the same input x will yield the 'perfect' policy value for this example (proof below)\n",
    "## W2 ready :)\n",
    "\n",
    "\n",
    "\n",
    "# W1 update\n",
    "dh = np.dot(dy, model['W2'])\n",
    "dh[h==0] = 0\n",
    "## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "dw1 = np.outer(dh, x)\n",
    "## W1 ready :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests: \n",
    "#### 1. After only updating W2 and evaluating policy gradient again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36783825 -0.12710085 -0.19478006  0.03333107 -0.60663095 -0.14226291\n",
      "   0.0986244  -0.40532996]\n",
      " [ 0.22222482  0.2931726  -0.02573194 -0.11987161  0.22036181 -0.01127969\n",
      "   0.51592645 -0.15751824]]\n"
     ]
    }
   ],
   "source": [
    "print(model['W2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86966992 -0.86966992]\n"
     ]
    }
   ],
   "source": [
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0031296 -0.0031296]\n"
     ]
    }
   ],
   "source": [
    "# W2 update\n",
    "dw2 = np.outer(dy, h)\n",
    "model['W2'] += dw2\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The policy gradient is 0 :)\n",
    "\n",
    "\n",
    "##### Of course, this update is crazy, as you force to perfectly fit the optimal policy for the current example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After only updating W1 and evaluating policy gradient again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.39760066 -0.39760066]\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H)\n",
    "\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 update\n",
    "dh = np.dot(dy, model['W2'])\n",
    "dh[h==0] = 0\n",
    "## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "dw1 = np.outer(dh, x)\n",
    "## W1 ready :)\n",
    "\n",
    "model['W1'] += dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23741854 -0.23741854]\n"
     ]
    }
   ],
   "source": [
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The policy gradient is not 0 because the RELU keeps the necessary neurons from activating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anyway, the final backprop code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(dy, h, x):\n",
    "    global model\n",
    "    # we are lucky that as we have dense layers, the gradient is linear\n",
    "\n",
    "    # W2 update\n",
    "    dw2 = np.outer(dy, h)\n",
    "    ## that means that if we ONLY update W2 += dw2, now forward propagating\n",
    "    ## the same input x will yield the 'perfect' policy value for this example (proof below)\n",
    "    ## W2 ready :)\n",
    "\n",
    "    # W1 update\n",
    "    dh = np.dot(dy, model['W2'])\n",
    "    dh[h<=0] = 0\n",
    "    ## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "    dw1 = np.outer(dh, x)\n",
    "    ## W1 ready :)\n",
    "    return dw1, dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Rewards\n",
    "As in Karpathy's Pong from Pixels, we normalize the rewards obtained in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S*H) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H*A)\n",
    "\n",
    "\n",
    "gamma = 0.97\n",
    "gamma_series = [1]\n",
    "for e in range(1000):\n",
    "    gamma_series.append(gamma_series[-1]*gamma + 1)\n",
    "\n",
    "\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S*H) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H*A)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "count_episodes = 0\n",
    "\n",
    "gradient_buffer = []\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "# Play a bunch of episodes\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    position_rewards= []\n",
    "\n",
    "\n",
    "    for e in range(100):\n",
    "        terminal = False\n",
    "        state = env.reset()\n",
    "        r = 0\n",
    "\n",
    "        while not terminal:\n",
    "            if  count_episodes %300==0:\n",
    "                #env.render()\n",
    "                pass\n",
    "\n",
    "\n",
    "            h,y,p = propagate_forward(state)\n",
    "\n",
    "            states.append(state)\n",
    "\n",
    "\n",
    "\n",
    "            # select action for current state\n",
    "            action = np.random.choice(2, p=p)\n",
    "            actions.append(action)\n",
    "\n",
    "            state, reward, terminal, info = env.step(action)\n",
    "            # we want to keep the pole close to the center of the screen\n",
    "            position_rewards.append(-np.abs(state[0])*2)\n",
    "\n",
    "            r+= 1\n",
    "\n",
    "\n",
    "        count_episodes+=1\n",
    "\n",
    "        if  count_episodes %10000==0:\n",
    "            print(model['W1'])\n",
    "            print(model['W2'])\n",
    "\n",
    "        rewards.append(r)\n",
    "\n",
    "    print('Average episode lasted: ',np.mean(rewards))\n",
    "    print('Number of episodes played: ',count_episodes)\n",
    "    rs = [list(reversed(gamma_series[:r])) for r in rewards]\n",
    "    mean_r = np.mean([np.mean(r) for r in rs])\n",
    "    rs = np.concatenate(rs) + np.array(position_rewards)\n",
    "    normalized_rewards = (rs - np.mean(rs))/np.std(rs)\n",
    "\n",
    "\n",
    "    # train\n",
    "\n",
    "    ind = np.arange(len(rs))\n",
    "    np.random.shuffle(ind)\n",
    "    for t in ind:\n",
    "        reward = normalized_rewards[t]\n",
    "        state = states[t]\n",
    "        action = actions[t]\n",
    "\n",
    "        h,y,p = propagate_forward(state)\n",
    "        dy = policy_gradient(action,reward, y, p)\n",
    "        dw1, dw2 = backprop(dy, h, state)\n",
    "\n",
    "        #update weights (train)\n",
    "        w1_update = np.clip(learning_rate * dw1, -0.0005, +0.0005)\n",
    "        w2_update = np.clip(learning_rate * dw2, -0.0005, +0.0005)\n",
    "\n",
    "        model['W1'] += w1_update\n",
    "        model['W2'] += w2_update\n",
    "\n",
    "    #gradient_buffer.append([np.linalg.norm(learning_rate * dw1), np.linalg.norm(learning_rate * dw2)])\n",
    "    #gradient_buffer.append([np.max(np.abs(learning_rate * dw1)), np.max(np.abs(learning_rate * dw2))])\n",
    "    #print(np.linalg.norm(dw1))\n",
    "    #print(np.linalg.norm(dw2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
