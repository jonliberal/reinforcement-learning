{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Xavier initialization\n",
    "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network.\n",
    "\n",
    "If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all.\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between:\n",
    "    sqrt(6) / sqrt(n_i + n_o)\n",
    "    \n",
    "#### Even better?\n",
    "\n",
    "In their 2015 paper, He et. al. demonstrated that deep networks (e.g. a 22-layer CNN) would converge much earlier if the following input weight initialization strategy is employed:\n",
    "\n",
    "  - Create a tensor with the dimensions appropriate for a weight matrix at a given layer, and populate it with numbers randomly chosen from a standard normal distribution.\n",
    "  - Multiply each randomly chosen number by √2/√n where n is the number of incoming connections coming into a given layer from the previous layer’s output (also known as the “fan-in”).\n",
    "  - Bias tensors are initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define environment and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create cart pole environment\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# get main parameters from the agent-environment interaction\n",
    "\n",
    "number_of_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "H = 8 \n",
    "A = number_of_actions\n",
    "S = state_dim\n",
    "\n",
    "# Model weights\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define useful functions for training and evaluating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    if max(y) > 10:\n",
    "        print(\"WARNING: NUMEROS GORDACOS\")\n",
    "        print(y)\n",
    "    return np.exp(y)/np.sum(np.exp(y))\n",
    "\n",
    "def propagate_forward(x):\n",
    "    global model\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0\n",
    "    y = np.dot(model['W2'], h)\n",
    "    p = softmax(y)\n",
    "\n",
    "    return h,y,p\n",
    "\n",
    "def policy_gradient(action, reward, y, p):\n",
    "    grad = -p\n",
    "    grad[action] += 1.\n",
    "    return grad*reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input\n",
    "x = np.array([-0.18903734, -1.75218701,  0.21354577,  2.74911676])\n",
    "\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are lucky that as we have dense layers, the gradient is linear\n",
    "\n",
    "# W2 update\n",
    "dw2 = np.outer(dy, h)\n",
    "## that means that if we ONLY update W2 += dw2, now forward propagating\n",
    "## the same input x will yield the 'perfect' policy value for this example (proof below)\n",
    "## W2 ready :)\n",
    "\n",
    "\n",
    "\n",
    "# W1 update\n",
    "dh = np.dot(dy, model['W2'])\n",
    "dh[h==0] = 0\n",
    "## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "dw1 = np.outer(dh, x)\n",
    "## W1 ready :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests: \n",
    "#### 1. After only updating W2 and evaluating policy gradient again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27323657  0.15243403 -0.36531208  0.04196286  0.37061983  0.22093565\n",
      "  -0.22469356  0.17293585]\n",
      " [ 0.16214547 -0.49864699  0.71875238  0.72459368  0.05994918  0.1239246\n",
      "   0.065119    0.18102478]]\n"
     ]
    }
   ],
   "source": [
    "print(model['W2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.39907043 -0.39907043]\n"
     ]
    }
   ],
   "source": [
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.31339330e-08 -8.31339329e-08]\n"
     ]
    }
   ],
   "source": [
    "# W2 update\n",
    "dw2 = np.outer(dy, h)\n",
    "model['W2'] += dw2\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The policy gradient is 0 :)\n",
    "\n",
    "\n",
    "##### Of course, this update is crazy, as you force to perfectly fit the optimal policy for the current example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After only updating W1 and evaluating policy gradient again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.38516315 -0.38516315]\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H)\n",
    "\n",
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 update\n",
    "dh = np.dot(dy, model['W2'])\n",
    "dh[h==0] = 0\n",
    "## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "dw1 = np.outer(dh, x)\n",
    "## W1 ready :)\n",
    "\n",
    "model['W1'] += dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09442277 -0.09442277]\n"
     ]
    }
   ],
   "source": [
    "h,y,p = propagate_forward(x)\n",
    "dy = policy_gradient(0,1, y, p)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The policy gradient is not 0 because the RELU keeps the necessary neurons from activating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anyway, the final backprop code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(dy, h, x):\n",
    "    global model\n",
    "    # we are lucky that as we have dense layers, the gradient is linear\n",
    "\n",
    "    # W2 update\n",
    "    dw2 = np.outer(dy, h)\n",
    "    ## that means that if we ONLY update W2 += dw2, now forward propagating\n",
    "    ## the same input x will yield the 'perfect' policy value for this example (proof below)\n",
    "    ## W2 ready :)\n",
    "\n",
    "    # W1 update\n",
    "    dh = np.dot(dy, model['W2'])\n",
    "    dh[h<=0] = 0\n",
    "    ## now we have how much we need to increase the hidden state: h = h+lr*dh\n",
    "    dw1 = np.outer(dh, x)\n",
    "    ## W1 ready :)\n",
    "    return dw1, dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Rewards\n",
    "As in Karpathy's Pong from Pixels, we normalize the rewards obtained in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode lasted:  22.8\n",
      "Number of episodes played:  100\n",
      "Average episode lasted:  22.79\n",
      "Number of episodes played:  200\n",
      "Average episode lasted:  20.93\n",
      "Number of episodes played:  300\n",
      "Average episode lasted:  21.82\n",
      "Number of episodes played:  400\n",
      "Average episode lasted:  19.57\n",
      "Number of episodes played:  500\n",
      "Average episode lasted:  20.67\n",
      "Number of episodes played:  600\n",
      "Average episode lasted:  23.45\n",
      "Number of episodes played:  700\n",
      "Average episode lasted:  20.91\n",
      "Number of episodes played:  800\n",
      "Average episode lasted:  21.9\n",
      "Number of episodes played:  900\n",
      "Average episode lasted:  22.17\n",
      "Number of episodes played:  1000\n",
      "Average episode lasted:  21.77\n",
      "Number of episodes played:  1100\n",
      "Average episode lasted:  24.32\n",
      "Number of episodes played:  1200\n",
      "Average episode lasted:  21.06\n",
      "Number of episodes played:  1300\n",
      "Average episode lasted:  20.48\n",
      "Number of episodes played:  1400\n",
      "Average episode lasted:  22.24\n",
      "Number of episodes played:  1500\n",
      "Average episode lasted:  22.35\n",
      "Number of episodes played:  1600\n",
      "Average episode lasted:  21.84\n",
      "Number of episodes played:  1700\n",
      "Average episode lasted:  24.2\n",
      "Number of episodes played:  1800\n",
      "Average episode lasted:  22.33\n",
      "Number of episodes played:  1900\n",
      "Average episode lasted:  23.13\n",
      "Number of episodes played:  2000\n",
      "Average episode lasted:  23.23\n",
      "Number of episodes played:  2100\n",
      "Average episode lasted:  20.29\n",
      "Number of episodes played:  2200\n",
      "Average episode lasted:  21.88\n",
      "Number of episodes played:  2300\n",
      "Average episode lasted:  22.01\n",
      "Number of episodes played:  2400\n",
      "Average episode lasted:  23.69\n",
      "Number of episodes played:  2500\n",
      "Average episode lasted:  24.6\n",
      "Number of episodes played:  2600\n",
      "Average episode lasted:  24.34\n",
      "Number of episodes played:  2700\n",
      "Average episode lasted:  22.67\n",
      "Number of episodes played:  2800\n",
      "Average episode lasted:  21.31\n",
      "Number of episodes played:  2900\n",
      "Average episode lasted:  21.64\n",
      "Number of episodes played:  3000\n",
      "Average episode lasted:  23.45\n",
      "Number of episodes played:  3100\n",
      "Average episode lasted:  23.22\n",
      "Number of episodes played:  3200\n",
      "Average episode lasted:  21.75\n",
      "Number of episodes played:  3300\n",
      "Average episode lasted:  22.12\n",
      "Number of episodes played:  3400\n",
      "Average episode lasted:  23.88\n",
      "Number of episodes played:  3500\n",
      "Average episode lasted:  21.68\n",
      "Number of episodes played:  3600\n",
      "Average episode lasted:  22.6\n",
      "Number of episodes played:  3700\n",
      "Average episode lasted:  23.85\n",
      "Number of episodes played:  3800\n",
      "Average episode lasted:  24.27\n",
      "Number of episodes played:  3900\n",
      "Average episode lasted:  20.7\n",
      "Number of episodes played:  4000\n",
      "Average episode lasted:  22.29\n",
      "Number of episodes played:  4100\n",
      "Average episode lasted:  23.51\n",
      "Number of episodes played:  4200\n",
      "Average episode lasted:  23.26\n",
      "Number of episodes played:  4300\n",
      "Average episode lasted:  23.7\n",
      "Number of episodes played:  4400\n",
      "Average episode lasted:  21.7\n",
      "Number of episodes played:  4500\n",
      "Average episode lasted:  22.98\n",
      "Number of episodes played:  4600\n",
      "Average episode lasted:  24.74\n",
      "Number of episodes played:  4700\n",
      "Average episode lasted:  21.45\n",
      "Number of episodes played:  4800\n",
      "Average episode lasted:  23.48\n",
      "Number of episodes played:  4900\n",
      "Average episode lasted:  22.74\n",
      "Number of episodes played:  5000\n",
      "Average episode lasted:  25.84\n",
      "Number of episodes played:  5100\n",
      "Average episode lasted:  23.95\n",
      "Number of episodes played:  5200\n",
      "Average episode lasted:  24.47\n",
      "Number of episodes played:  5300\n",
      "Average episode lasted:  24.75\n",
      "Number of episodes played:  5400\n",
      "Average episode lasted:  25.06\n",
      "Number of episodes played:  5500\n",
      "Average episode lasted:  25.4\n",
      "Number of episodes played:  5600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d3fd71e0b9a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#update weights (train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mw1_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mw2_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw1_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m     \"\"\"\n\u001b[0;32m-> 2084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         return _clip_dep_invoke_with_casting(\n\u001b[0;32m--> 132\u001b[0;31m             um.clip, a, min, max, out=out, casting=casting, **kwargs)\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[0;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# try to deal with broken casting rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_UFuncOutputCastingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Numpy 1.17.0, 2019-02-24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S*H) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H*A)\n",
    "\n",
    "\n",
    "gamma = 0.97\n",
    "gamma_series = [1]\n",
    "for e in range(1000):\n",
    "    gamma_series.append(gamma_series[-1]*gamma + 1)\n",
    "\n",
    "\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H,S) / np.sqrt(S*H) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(A,H) / np.sqrt(H*A)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "count_episodes = 0\n",
    "\n",
    "gradient_buffer = []\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "# Play a bunch of episodes\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    position_rewards= []\n",
    "\n",
    "\n",
    "    for e in range(100):\n",
    "        terminal = False\n",
    "        state = env.reset()\n",
    "        r = 0\n",
    "\n",
    "        while not terminal:\n",
    "            if  count_episodes %300==0:\n",
    "                #env.render()\n",
    "                pass\n",
    "\n",
    "\n",
    "            h,y,p = propagate_forward(state)\n",
    "\n",
    "            states.append(state)\n",
    "\n",
    "\n",
    "\n",
    "            # select action for current state\n",
    "            action = np.random.choice(2, p=p)\n",
    "            actions.append(action)\n",
    "\n",
    "            state, reward, terminal, info = env.step(action)\n",
    "            # we want to keep the pole close to the center of the screen\n",
    "            position_rewards.append(-np.abs(state[0])*2)\n",
    "\n",
    "            r+= 1\n",
    "\n",
    "\n",
    "        count_episodes+=1\n",
    "\n",
    "        if  count_episodes %10000==0:\n",
    "            print(model['W1'])\n",
    "            print(model['W2'])\n",
    "\n",
    "        rewards.append(r)\n",
    "\n",
    "    print('Average episode lasted: ',np.mean(rewards))\n",
    "    print('Number of episodes played: ',count_episodes)\n",
    "    rs = [list(reversed(gamma_series[:r])) for r in rewards]\n",
    "    mean_r = np.mean([np.mean(r) for r in rs])\n",
    "    rs = np.concatenate(rs) + np.array(position_rewards)\n",
    "    normalized_rewards = (rs - np.mean(rs))/np.std(rs)\n",
    "\n",
    "\n",
    "    # train\n",
    "\n",
    "    ind = np.arange(len(rs))\n",
    "    np.random.shuffle(ind)\n",
    "    for t in ind:\n",
    "        reward = normalized_rewards[t]\n",
    "        state = states[t]\n",
    "        action = actions[t]\n",
    "\n",
    "        h,y,p = propagate_forward(state)\n",
    "        dy = policy_gradient(action,reward, y, p)\n",
    "        dw1, dw2 = backprop(dy, h, state)\n",
    "\n",
    "        #update weights (train)\n",
    "        w1_update = np.clip(learning_rate * dw1, -0.0005, +0.0005)\n",
    "        w2_update = np.clip(learning_rate * dw2, -0.0005, +0.0005)\n",
    "\n",
    "        model['W1'] += w1_update\n",
    "        model['W2'] += w2_update\n",
    "\n",
    "    #gradient_buffer.append([np.linalg.norm(learning_rate * dw1), np.linalg.norm(learning_rate * dw2)])\n",
    "    #gradient_buffer.append([np.max(np.abs(learning_rate * dw1)), np.max(np.abs(learning_rate * dw2))])\n",
    "    #print(np.linalg.norm(dw1))\n",
    "    #print(np.linalg.norm(dw2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
